{
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "AKGfyxWhAJui"
      },
      "cell_type": "markdown",
      "source": "# Naive Bayes\n\nIn contrast to *k*-means clustering, Naive Bayes is a supervised machine-learning (ML) algorithm. It provides good speed and good accuracy and is often used in aspects of natural-language processing such text classification or, in our case in this section, spam detection.\n\nSpam emails are more than just a nuisance. As recently as 2008, spam constituted an apocalyptic 97.8 percent of all email traffic according to a [2009 Microsoft security report](http://download.microsoft.com/download/4/3/8/438BE24D-4D58-4D9A-900A-A1FC58220813/Microsoft_Security_Intelligence_Report _volume8_July-Dec2009_English.pdf). That tide has thankfully turned and, as of May 2019, spam makes up only about [85 percent of email traffic](https://www.talosintelligence.com/reputation_center/email_rep) — thanks, in no small part, to Naive Bayes spam filters.\n\nNaive Bayes is a convenient algorithm for spam detection because it does not require encoding complex rules. All it needs is training examples, of which there are plenty when it comes to email spam. Naive Bayes does all this through the use of [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability).\n\n> **Learning objective:** By the end of this section, you should have a basic understanding of how naive Bayes works and some of the reasons for its popularity.\n\n## Conditional probability\n\nOrdinary probability deals with the likelihood of isolated events occurring. For example, rolling a 6 on a fair six-sided die will occur, on average, on one out of six rolls. Mathematicians express this probability as $P({\\rm die}=6)=\\frac{1}{6}$.\n\nConditional probability concerns itself with the contingencies of interconnected events: what is the probability of event $A$ happening if event $B$ occurs. Mathematicians denote this as $P(A|B)$, or \"the probability of $A$ given $B$.\"\n\nIn order to compute the probability of conditional events, we use the following equation:\n\n$P(A \\mid B)=\\cfrac{P(A \\cap B)}{P(B)}$\n\nThis equation is nice, but it assumes that we know the joint probability $P(A\\cap B)$, which we often don't. Instead, we often need to know something about $A$ but all we can directly observe is $B$. For instance, when we want to infer whether an email is spam only by knowing the words it contains. For this, we need Bayes' law.\n\n## Bayes' law\n\nBayes' law takes its name from the eighteenth-century English statistician and philosopher Thomas Bayes, who described the probability of an event based solely on prior knowledge of conditions that might be related to that event thus:\n\n$P(A \\mid B)=\\cfrac{P(B \\mid A)P(A)}{P(B)}$\n\nIn words, Bayes' Law says that if I know the prior probabilities $P(A)$ and $P(B)$, in addition to the likelihood (even just an assumed likelihood) $P(B \\mid A)$, I can compute the posterior probability $P(A \\mid B)$. Let's apply this to spam.\n\n<img align=\"center\" style=\"padding-right:10px;\" src=\"Images/spam.png\" border=\"5\">\n\nIn order to use Bayesian probability on spam email messages like this one, consider it (and all other emails, spam or ham) to be bags of words. We don't care about word order or even word meaning. We just want to count the frequency of certain words in spam messages versus the frequency of those same words in valid email messages.\n\nLet's say that, after having counted the words in hundreds of emails that we have received, we determine the probability of the word \"debt\" appearing in any kind of email message (spam or ham) to be 0.157, with the probability of \"debt\" appearing in spam messages being 0.309. Further more, let's say that we assume that there is a 50 percent chance that any given email message we receive is spam (for this example, we don't know either way what type of email it might be, so it's a coin flip). Mathematically, we could thus say:\n - Probability that a given message is spam: $P({\\rm S})=0.5$\n - Probability that “debt” appears in a given message: $P({\\rm debt})=0.157$\n - Probability that “debt” appears in a spam message: $P({\\rm debt} \\mid {\\rm S})=0.309$\n\nPlugging this in to Bayes' law, we get the following probability that an email message containing the word \"debt\" is spam:\n\n$P({\\rm S} \\mid {\\rm debt})=\\cfrac{P({\\rm debt} \\mid {\\rm S})P({\\rm S})}{P({\\rm debt})}=\\cfrac{(0.309)(0.5)}{0.157}=\\cfrac{0.1545}{0.157}=0.984$\n\nThus if an email contains the word \"debt,\" we calculate that it is 98.4 percent likely to be spam.\n\n## What makes it naive?\n\nOur above calculation is great for looking at individual words, but emails contain several words that can give us clues to an email's relative likelihood of being spam or ham. For example, say we wanted to determine whether an email is spam given that it contains the words \"debt\" and \"bills.\" We can begin by reasoning that the probability that an email containing \"debt\" and \"bills\" is spam is, if not equal, at least proportional to the probability of \"debt\" and \"bills\" appearing in known spam messages times the probability of any given message being spam:\n\n$P({\\rm S} \\mid {\\rm debt, bills}) \\propto P({\\rm debt, bills} \\mid {\\rm S})P({\\rm S})$\n\n(**Mathematical note:** The symbol ∝ represents proportionality rather than equality.)\n\nNow if we assume that the occurrence of the words \"debt\" and \"bills\" are independent events, we can extend this proportionality:\n\n$P({\\rm S} \\mid {\\rm debt, bills}) \\propto P({\\rm debt} \\mid {\\rm S})P({\\rm bills} \\mid {\\rm S})P({\\rm S})$\n\nWe should state here that this assumption of independence is generally not true. Just look at the example spam message above. The probability that \"bills\" will appears in a spam message containing \"debt\" is probably quite high. However, assuming that the probabilities of words occurring in our email messages are independent is useful and works surprising well. This assumption of independence is the naive part of the Baysian probabilities that we will use in this section; expressed mathematically, the working assumption that will underpin the ML in this section is that for any collection of $n$ words:\n\n$P({\\rm S}\\mid {\\rm word_1}, {\\rm word_2},\\ldots, {\\rm word}_n)=P({\\rm S})P({\\rm word_1}\\mid {\\rm S})P({\\rm word_2}\\mid {\\rm S})\\cdots P({\\rm word}_n\\mid {\\rm S})$\n\n> **Key takeaway:** We cannot emphasize enough that this chain rule expressed in the equation above—that the probability of a message being spam based on the words in it is equal to the product of the likelihoods of those individual words appearing in messages known to be spam is ***not*** true. But it gets good results and, in the world of data science, fast and good enough always trump mathematical fidelity."
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Rn-ySwNfU234"
      },
      "cell_type": "markdown",
      "source": "## Import the dataset\n\nIn this section, we'll use the [SMS Spam Collection dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). It contains 5,574 messages collected for SMS spam research and tagged as \"spam\" or \"ham.\" The dataset files contain one message per line with each line being composed of the tag and the raw text of the SMS message. For example:\n\n| Class | Message                       |\n|:------|:------------------------------|\n| ham   | What you doing?how are you?   |\n| ham   | Ok lar... Joking wif u oni... |\n\nLet’s now import pandas and load the dataset. (Note that the path name is case sensitive.)"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DOOEodQVAJuz",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd \ndf = pd.read_csv('Data/SMSSpamCollection', sep='\\t', names=['Class', 'Message'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> What do the `sep` and `names` parameters do in the code cell above? (**Hint:** If you are unsure, you can refer to the built-in Help documentation using `pd.read_csv?` in the code cell below.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's take an initial look at what's in the dataset."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "colab_type": "code",
        "id": "6j8BELX6AJu4",
        "outputId": "ab88082e-d6d5-419d-d20b-1f5dc2db1b6e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t7jxNfreWkrj"
      },
      "cell_type": "markdown",
      "source": "Note that several entries in the `Message` column are truncated. We can use the `set_option()` function to set pandas to display the maximum width of each entry. "
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "colab_type": "code",
        "id": "gB50lS3vVgoY",
        "outputId": "29a66a1b-7bc7-47ce-aeb0-15de4f35f392",
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.set_option('display.max_colwidth', -1)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IdfsM1iQXGV6"
      },
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> What do you think the purpose of the `-1` parameter passed to `pd.set_option()` is in the code cell above?  \n\nAlternatively, we can dig into individual messages."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "Iu9QH5WgW_kh",
        "outputId": "3fac377e-a55b-4a4e-94cc-934e7c814e8b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df['Message'][13]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L0dtTJlf055l"
      },
      "cell_type": "markdown",
      "source": "## Explore the data\n\nNow that we have an idea of some of the individual entries in the dataset, let's get a better sense of the dataset as a whole."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "colab_type": "code",
        "id": "VyPlozWXAJvB",
        "outputId": "66461b0a-795c-4eb7-ca1f-619a78a199b2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Now run the `describe()` method on `df`. Does it provide much useful information about this dataset? If not, why not?"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Possible exercise solution**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also visualize the dataset to graphically see the mix of spam to ham. (Note that we need to include the `%matplotlib inline` magic command in order to actually see the bar chart here in the notebook.)"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "colab_type": "code",
        "id": "TiplFkJIXqBj",
        "outputId": "8c346b5c-1466-4d00-c67d-a1ae7d32491e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\ndf.groupby('Class').count().plot(kind='bar')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vfYFhiy5zsJl"
      },
      "cell_type": "markdown",
      "source": "> **Key takeaway:** Notice that here an in previous sections we have stuck together several methods to run on a `DataFrame`. This kind of additive method-stacking is part of what makes Python and pandas such a power combination for the rough-and-ready data exploration that is a crucial part of data science.\n\n## Explore the data using word clouds\n\nBecause our data is largely not numeric, you might have noticed that some of our go-to data exploration tools (such as bar charts and the `describe()` method) have been of limited use in exploring this data. Instead, word clouds can be a powerful way of getting a quick glance at what's represented in text data as a whole."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install wordcloud",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will have to supply a number of parameters to the `WordCloud()` function and to matplotlib in order to render the word clouds, so we will save ourselves some redundant work by writing a short function to handle it. Parameters for `WordCloud()` will include the stop words we want to ignore and font size for the words in the cloud. For matplotlib, these parameters will include instructions for rendering the word cloud."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fBszilkex2DR",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\ndef get_wordcloud(text_data,title):\n  wordcloud = WordCloud(background_color='black',\n                        stopwords=set(STOPWORDS),\n                        max_font_size=40, \n                        relative_scaling=1.0,\n                        random_state=1\n  ).generate(str(text_data))\n\n  fig = plt.figure(1, figsize=(12, 12))\n  plt.axis('off')\n  plt.title(title)\n  plt.imshow(wordcloud)\n  plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now it is time to plot the word clouds."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "colab_type": "code",
        "id": "yrBK9Sa93Sko",
        "outputId": "bd8ce0a6-e2ec-47a6-afd0-61bdbed3752d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "spam_msg = df.loc[df['Class']=='spam']['Message']\nget_wordcloud(spam_msg,'Spam Cloud')\nham_msg = df.loc[df['Class']=='ham']['Message']\nget_wordcloud(ham_msg,'Ham Cloud')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "16QKWJcH4xWz"
      },
      "cell_type": "markdown",
      "source": "Looking at the two word clouds, it is immediately apparent that the frequency of the most common words is different between our spam and our ham messages, which will form the primary basis of our spam detection.\n\n## Explore the data numerically\n\nJust because the data does not naturally lend itself to numerical analysis \"out of the box\" does not mean that we can't do so. We can also analyze the average length of spam and ham messages to see if there are differences. For this, we need to create a new column. "
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "colab_type": "code",
        "id": "RNdBk6FY54B5",
        "outputId": "70d5e8cf-7877-4a31-d937-2bfd6ad1e90a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df['Length_of_msg'] = df['Message'].apply(len)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> What does the `apply()` method do in the code cell above? (**Hint:** If you are unsure, you can refer to [this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html).)\n\nNow that we have the length of each message, we can visualize those message lengths using a histogram."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "colab_type": "code",
        "id": "B2Z5qyR36Wmi",
        "outputId": "dedecd52-e973-42b2-c52f-74c42286b50e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.groupby('Class')['Length_of_msg'].plot(kind='hist', bins=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YysPkl9C_Ie3"
      },
      "cell_type": "markdown",
      "source": "The orange histogram is the spam messages. Because there are so many more ham messages than spam, let's break these out separately to see the details more clearly."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "colab_type": "code",
        "id": "e8-yMAck9HcF",
        "outputId": "6fa17b14-f40a-48a7-cb14-b3de203adbc4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.hist(bins=50,by='Class', column='Length_of_msg')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Spam messages skew much longer than ham messages.\n\n> **Question**\n>\n> Why does it appear in the details histograms that there is almost no overlap between the lengths of ham and spam text messages? What do the differences in scale tell us (and what could they inadvertently obscure)?\n\nLet's look at the differences in length of the two classes of message numerically."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "colab_type": "code",
        "id": "Ahszzxkj9v8i",
        "outputId": "392f2cd3-0311-4b6b-aaf6-a6d74b05ba97",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.groupby('Class').mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SlxBSNPh-WzJ"
      },
      "cell_type": "markdown",
      "source": "These numbers accord with what we saw in the histograms.\n\nNow, let's get to the actual modeling and spam detection."
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "-b09i3ZU_viQ"
      },
      "cell_type": "markdown",
      "source": "## Prepare the data for modeling\n\nOne of the great strengths of naive Bayes analysis is that we don't have to go too deep into text processing in order to develop robust spam detection. However, the text is raw and it does require a certain amount of cleaning. To do this, we will use one of the most commonly used text-analytics libraries in Python, the Natural Language Toolkit (NLTK). However, before we can import it, we will need to first install it. "
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "w4D4vg0vFc-i",
        "outputId": "3c9d7cfb-1998-4c57-e14d-355ac489def2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can now import NLTK, in addition to the native Python string library to help with our text manipulation. We will also download the latest list of stop words (such as 'the', 'is', and 'are') for NLTK."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "colab_type": "code",
        "id": "V4s7jcVMFwDn",
        "outputId": "ad7a10c1-76df-4b4e-aa1f-f6f01ed46ec9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import string\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WqcfZ4zWSwHG"
      },
      "cell_type": "markdown",
      "source": "Part of our data preparation will be *vectorizing* the text data. Recall that earlier in the section when we first introduced naive Bayes analysis, we stated that we wanted to treat our messages as \"bags of words\" rather than as English-language messages. Vectorization is the process by which we convert our collection of text messages to a matrix of word counts.\n\nPart of the vectorization process will be for us to remove punctuation from the messages and exclude stop words from our analysis. We will write a function to perform those tasks here, because we will want to access those actions later on."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pMLY0MR7DCmn",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def txt_preprocess(text):\n   \n    #Remove punctuation \n    temp = [w for w in text if w not in string.punctuation]\n    temp = ''.join(temp)\n    \n    #Exclude stopwords\n    processedtext = [w for w in temp.split() if w.lower() not in stopwords.words('english')]\n    return processedtext",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rxMogwVEeQGQ"
      },
      "cell_type": "markdown",
      "source": "Scikit-learn provides a count-vectorizer function. We will now import it and then use the `txt_preprocess()` function we just wrote as a custom analyzer for it."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Shw3GmFe-mh-",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\n\nX = df['Message']\ny = df['Class']\n\nCountVect = CountVectorizer(analyzer=txt_preprocess).fit(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F2IzaX-SUqY-"
      },
      "cell_type": "markdown",
      "source": "> **Technical note:** The convention of using an upper-case `X` to represent the independent variables (the predictors) and a lower-case `y` to represent the dependent variable (the response) comes from statistics and is commonly used by data scientists.\n\nIn order to see how the vectorizer transformed the words, let's check it against a common English word like \"go.\""
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "ZQzl0gqIKBSx",
        "outputId": "68b4ef4a-2a9e-4448-e220-80ed37020f80",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(CountVect.vocabulary_.get('go'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KrcmMuVZVBYq"
      },
      "cell_type": "markdown",
      "source": "So \"go\" appears 6,864 times in our dataset.\n\nNow, before we transform the entire dataset and train the model, we have the final preparatory step of splitting our data into training and test data to perform."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xZfa9xSZe80l",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, we will transform our training messages into a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix). \"Document\" might sound a little grandiose in this case as it refers to individual text messages, but it is a term of art for text analysis."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_PlppjOHgM29",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train_data = CountVect.transform(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DNipADIdV8MK"
      },
      "cell_type": "markdown",
      "source": "This can be a tricky concept, so let's look at the training-text matrix directly."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "colab_type": "code",
        "id": "gE9dK247Oywp",
        "outputId": "4bd1d547-ed09-4187-f935-46560cc8362e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(X_train_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "8PXoU8jAMpH7",
        "outputId": "53932f19-0a7e-4cf4-dfb8-127e1b63a52e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`X_train_data` is now a 3900x11425 matrix, where each of the 3,900 rows represents a text (\"document\") from the training dataset and each column is a specific word (11,425 of them in this case).\n\n> **Key takeaway:** Putting our bag of words into a document-term matrix like this is a standard tool of natural-language processing and text analysis, and it is used in contexts beyond naive Bayes analysis in which word-frequency is important, such as [term frequency–inverse document frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the model\n\nNow it is time to train our naive Bayes model. For our model, we will use the multinomial naive Bayes classifier. \"Multinomial\" in this case derives from our assumption that, for our bag of $n$ words, $P({\\rm S}\\mid {\\rm word_1}, {\\rm word_2},\\ldots, {\\rm word}_n)=P({\\rm S})P({\\rm word_1}\\mid {\\rm S})P({\\rm word_2}\\mid {\\rm S})\\cdots P({\\rm word}_n\\mid {\\rm S})$ and that we don't assume that our word likelihoods follow a normal distribution."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RpnhqNNzg18q",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import MultinomialNB",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "INtPpg9NWlIs",
        "outputId": "ff7304d6-321b-4bbc-b314-54a7faf044d8",
        "trusted": true
      },
      "cell_type": "code",
      "source": "naivebayes_model = MultinomialNB()\nnaivebayes_model.fit(X_train_data,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "t72QOLCJW-td"
      },
      "cell_type": "markdown",
      "source": "Our model is now fitted. However, before we run our predictions on all of our test data, let's see what our model says about some artificial data in order to get a better sense of what our model will do with all of the messages in our test dat. From the word clouds we constructed earlier, we can see that \"call\" and \"free\" are both prominent words among our spam messages, so let's create our own spam message and see how our model classifies it."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jG1xRH5WZHmc",
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = naivebayes_model.predict(CountVect.transform(['Get 50% off your next purchase. Call now']))\npred",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ae42o56MZ3Xw"
      },
      "cell_type": "markdown",
      "source": "As we expected, our model correctly classified this message as spam. \n\n> **Exercise**\n>\n> Review the ham word cloud above, construct a ham message, and then run it against the model to see how it is classified."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Possible exercise solution**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred2 = naivebayes_model.predict(CountVect.transform(['Let me know what time we should go.']))\npred2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's run our test data through the model. First, we need to transform it to a document-term matrix."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UyQ930-pWp6B",
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_test_data = CountVect.transform(X_test)\nX_test_data.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Run the predictions for the test data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise solution**"
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QLIohljRhSlV",
        "trusted": true
      },
      "cell_type": "code",
      "source": "predictions = naivebayes_model.predict(X_test_data)\npredictions",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jgSq8jKHaOxt"
      },
      "cell_type": "markdown",
      "source": "Now it's time to evaluate our model's performance."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "lUYcIykDndpe",
        "outputId": "be124dac-38c2-43fd-be32-47a6ddf461be",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(predictions, y_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lh3xo51bbjGc"
      },
      "cell_type": "markdown",
      "source": "> **Exercise**\n>\n> Overall, our model is  good for spam detection, but our recall score (the proportion of actual positives that were identified correctly) is surprisingly low. Why might this be? What implications does it have for spam detection? (**Hint:** Use the scikit-learn `confusion_matrix()` function to better understand the specific performance of the model. For help interpreting the confusion matrix, see [this page](https://en.wikipedia.org/wiki/Confusion_matrix).)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Possible exercise solution**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(confusion_matrix(y_test, predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway**\n>\n> The performance of our naive Bayes model helps underscore the algorithm's popularity, particularly for spam detection. Even untuned, we got good performance, performance that would only continue to improve in production as users submitted more examples of spam messages."
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "block1_nb.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}