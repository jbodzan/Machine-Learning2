{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Azure Machine Learning Studio\n\nThus far in this track, we have examined building and fitting machine-learning (ML) models “locally.” True, the notebooks have been located in the cloud themselves, but the models with all of their predictive and classification power are stuck in those notebooks. To use these models, you would have to load data into your notebooks and get the results there.\n\nIn practice, we want those models to be accessible from a number of locations. And while the management of production ML models has a lifecycle all its own, one part of that is making models accessible from the web. One way to do so is to develop them by using third-party cloud tools, such as [Microsoft Azure ML Studio](https://studio.azureml.net) (not to be confused with Microsoft Azure Machine Learning Service, which provides end-to-end lifecycle management for ML models).\n\nAlternatively, we can develop and deploy a function that can be accessed by other programs over the web—a web service—that runs within Azure ML Studio. Cloud-based ML-development tools like Azure ML Studio streamline the building, training, and deployment of machine-learning models by providing a drag-and-drop interface and making a wide array of ML algorithms and tools readily available. Crucially, such tools simplify the deployment and management of ML models in production (not a focus of this course, but a major consideration in the real world). Let's revisit our simple linear model from the last session, this time in Azure ML Studio.\n\n> **Learning goal:** By the end of this section, you should be comfortable building ML models in Azure ML Studio and publishing them as web services."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Save dataset locally\n\nWe can't access our dataset in Azure Notebooks directly from Azure ML Studio, so before we can use it, we will save it locally in order to upload it to Azure ML Studio.\n\n1. Go to the browser tab with the Azure Notebooks project for this course. Click the **Data** folder.\n\n2. Click the option button next to **UN11.csv**.\n\n3. At the top of the project page, click **Download**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/001.png\" border=\"5\">\n\n4. Save UN11.csv to a location on your computer that you can easily remember for later access."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Upload dataset\n\nNow we will go to Azure ML Studio and upload the UN11 dataset.\n\n1. Open [Azure ML Studio](https://studio.azureml.net) in a new browser tab and sign in with Microsoft credentials. Azure ML Studio is free and does not require an Azure subscription. Once signed in to your Microsoft account (with the same credentials you’ve used for Azure Notebooks), you're in your “workspace.”\n\n2. In the left pane, click **DATASETS** (the database drum-cluster icon).\n\n3. In the lower-right part of the screen, click **NEW** (the plus sign icon).\n\n   <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/005.png\" border=\"5\">\n\n4. In the **NEW** window, next to **DATASET**, click **FROM LOCAL FILE**.\n\n   <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/010.png\" border=\"5\">\n\n5. In the **Upload a new dataset** window, in the **SELECT THE DATA TO UPLOAD** field, click **Browse**. \n   - Navigate to the location where you saved the UN11.csv file you downloaded from Azure Notebooks.\n   - Select the file and click **Open**. \n   - Leave the default entries autofilled in the **Upload a new dataset** window and click **OK** (the check mark icon in the lower-right part of the screen).\n   "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Create a simple linear regression model\n\n1. In the left pane, click **Experiments** (the Erlenmeyer flask icon).\n\n2. In the lower-right part of the screen, click **New** (the plus sign icon).\n\n3. Under **Microsoft Samples**, click **Blank Experiment**.\n\n   <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/025.png\" border=\"5\">\n\n4. At the top of the new experiment canvas, click in the name field (which reads “Experiment created on [Date]”) and rename the experiment to something descriptive (such as \"Simple Linear Regression\").\n\n5. In the left panel, under **Saved Datasets > My Datasets**, click **UN11.csv** and drag it onto the canvas.\n\n   <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/030.png\" border=\"5\">\n   > <font color=red>**Technical note:**</font> You can type the name of any of the modules mentioned in these instructions into the Search experiment items field at the top of the left pane to simplify searching for them.\n\n6. In the left panel, under **Data Transformation > Manipulation**, click **Select Columns in Dataset** and drag it onto the canvas under the **UN11.csv** module.\n\n7. Connect the **UN11.csv** module to **Select Columns in Dataset**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/035.png\" border=\"5\">\n\n8. Click the **Select Columns in Dataset** module if it is not selected. We will use it to select the columns from the dataset to use in the model.\n    - In the **Properties** panel on the right, click the **Launch column selector** button.\n    - In the **Select columns** window, click the field next to the **AVAILABLE COLUMNS** pane and select **`lifeExpF`** and **`ppgdp`** and move them to the **SELECTED COLUMNS** pane. Click the check-mark icon.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/040.png\" border=\"5\">\n\n9. In the left panel, under **Machine Learning > Initialize Model > Regression**, click **Linear Regression** and drag it onto the canvas next to the **Select Columns in Dataset** module.\n\n10. Click the **Linear Regression** module. In the **Properties** pane on the right, in the **Random number seed** field enter **`42`**; leave all other default values as they are.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/045.png\" border=\"5\">\n\n11. In the left pane, under **Machine Learning > Train**, click **Train Model** and drag it onto the canvas under both the **Linear Regression** and **Select Columns in Dataset** modules.\n\n12. Connect **Linear Regression** to the upper-left part of the **Train Model** module, and then connect **Select Columns in Dataset** to the upper-right part of it.\n\n13. Click on the **Train Model** module so that .we will tell the model which column from the dataset should serve as response in the model.\n    - In the **Properties** panel on the right, click the **Launch column selector** button.\n    - In the **Select a single column** window, click the **Press enter to enter column name** field, and then select  **`lifeExpF`** and click the check-mark icon.\n    \n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/050.png\" border=\"5\">\n    \n14. In the left panel, under **Machine Learning > Score**, click **Score Model** and drag it onto the canvas under both the **Train Model** and **Select Columns in Dataset** modules.\n\n15. Connect **Train Model** to the upper-left part of the **Score Model** module, and then connect **Select Columns in Dataset** to the upper-right part of it. Leave the default **Properties** setting for the module.\n\n16. In the left pane, under **Machine Learning > Evaluate**, click **Evaluate Model** and drag it onto the canvas under the **Score Model** module.\n\n17. 17.\tConnect **Score Model** to the upper-left receptor of the **Evaluate Model** module.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/055.png\" border=\"5\">\n\n18. At the bottom of the window, click **Run**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/060.png\" border=\"5\">\n    \n19. After all of the modules have run, right-click the circle in the center at the bottom of the **Evaluate Model** module, and then select **Visualize**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/065.png\" border=\"5\">\n\nWith an $R^2$ score (referred to in Azure ML Studio as the *Coefficient of Determination*) of 0.30, we got the score that we would expect (identical to what we got in the previous section)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Manipulate the dataset\n\nLet's do what we did in Section 2.1 and transform `ppgdp` to account for the logarithmic relationship between per-capita GDP and female life expectancy.\n\n> **Technical note:** Now that you’re familiar with the Azure ML Studio interface, procedures that follow in this section will only be as detailed as they were above when we encounter new functionality and aspects of the interface.\n\n1. From **Statistical Functions** in the left panel, add **Apply Math Operation** between the **UN11.csv** and **Select Columns in Dataset** modules.\n\n2. Delete the connector between **UN11.csv** and **Select Columns in Dataset** and instead form the connections to run **UN11.csv -> Apply Math Operation -> Select Columns in Dataset**. To delete a connector, simply right-click it and select **Delete** from the context menu.\n\n3. Click the **Apply Math Operation** module.\n   - In the **Properties** panel on the right, from the **Category** drop-down menu, select **Basic**.\n   - In the **Basic math function** drop-down menu, select **Log10**.\n   - Click the **Launch column selector** button.\n   - In the **Select columns** window, click the drop-down menu that reads **column type** and then select **column name**. In the field that appear beside that drop-down menu, enter or select **`ppgdp`** and click the check-mark icon.\n   - In the **Output mode** drop-down menu, select **Append**.\n   \n4. Right-click the **Apply Math Operation** module and select **Run selected**.\n\n   <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/075.png\" border=\"5\">\n   \n5. Click the **Select Columns in Dataset** module and then replace **`ppgdp`** with **`Log10(ppgdp)`** as one of the selected columns.\n\n6. Rerun the model. After all of the modules have run, right-click the circle at the center-bottom of the **Evaluate Model** module, and then select **Visualize**.\n\nOur new model now has an $R^2$ score of 0.60, which is exactly what we expected."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Multiple regression revisited\n\nWe saw in Section 2.1 that we could extract more complex relationships in the data by including more predictors in our model. This is extremely easy to do in Azure ML Studio: all we have to do is include more columns in our model.\n\n> **Exercise**\n>\n> Go back into your experiment and alter it to include the `pctUrban` column. (**Hint:** Use the **Launch column selector** on the **Select Columns in Dataset** module. Be sure to include `Log10(ppgdp)` and `lifeExpF` in addition to `pctUrban`.)\n\n> **Question**\n>\n> What was your $R^2$ score for the enhanced model? Is it what you were expecting?\n\nOf course, as easy as it is to add columns, we should see what happens when we include all of the columns.\n\n1. Click the **Select Columns in Dataset** module and then the **Launch column selector** button.\n\n2. In the **Select columns** window in the **AVAILABLE COLUMNS** pane, select **region**, **group**, and **fertility**, move them to the **SELECTED COLUMNS** pane, and click the check-mark button.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/085.png\" border=\"5\">\n\n3. Run the experiment again and check the new $R^2$ score.\n\nClearly 0.81 is a much better $R^2$ score, which serves to demonstrate that there is a lot of information supplied by the other features. But how is Azure ML Studio handling the categorical features?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Categorical features\n\nTo see this, right-click in the center at the bottom of the **Train Model** module, and then select **Visualize**.\n\nIn the **Batch Linear Regressor** window, scroll down. What Azure ML Studio refers to as \"weights\" are what we referred to in this section as \"coefficients.\" The bias term is the intercept, and all of the other weights are the slopes. But what does \"slope\" even mean in the context of categorical features?\n\nAzure ML Studio uses a technique called [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) in which categorical features are assigned a state of `1` or `0` based on whether or not they are true or not. Thus an Organisation for Economic Co-operation and Developmen (OECD) member country would have a value of `1` for the `oecd` value in `group` and `0`s for `africa` and `other`.\n\nAnother term for this is *dummy variables*, and it might help to see it in action in Python here in the notebook. First, we'll need to import pandas and our dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\ndf = pd.read_csv('Data/UN11.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pandas has a function that automatically creates dummy variable for us, `get_dummies()`. Here is it in use on the `group` feature of our dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.get_dummies(data=df, columns=['group'])\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We no longer have a single `group` column, but rather columns for `group_africa`, `group_oecd`, and `group_other`.\n\n> **Exercise**\n>\n> Now use the `get_dummies()` function to create dummy variables for the `region` column and display the first five entries of the updated `DataFrame`.\n\n> **Exercise solution**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.get_dummies(data=df, columns=['region'])\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Technical note:** Note that each category within a categorical feature requires its own column for the dummy variables, each of which in turn constitutes its own new feature. If you have data with many categories in it, this can greatly increase the number of features you must deal with. For example, we just turned our dataset from one with six features into one with 15 features. Remember that as the number of features increases, so does the amount of data needed to adequately model it. The curse of dimensionality strikes again!\n\nBack to the model in Azure ML Studio. When it comes time to make predictions based on the model, all features get multiplied by their respective coefficients. The continuous numeric features (`fertility`, `ppgdp`, `lifeExpF`, and `pctUrban`) have their values multiplied by their weights, and the dummy variables have their `1` or `0` value multiplied by their weights."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Splitting data\n\nSo far, we have been committing a fairly serious data-science sin in the name of instructional clarity: we have not been splitting our data into test and training sets. This made sense while we were exploring the mechanics of linear regression, but now it's time to start handling our data correctly.\n\n1. On the experiment canvas in Azure ML Studio, delete the connections between **Select Columns in Dataset** and the **Train Model** and **Score Model** modules.\n\n2. From the left pane, under **Data Transformation > Sample and Split**, drag **Split Data** under the **Select Columns in Dataset** module.\n\n3. In the **Properties** pane for the **Split Data** module, enter `0.7` for **Fraction of rows in the first output dataset**, and enter `42` for **Random seed**. Leave the default values for all other settings.\n\n4. Connect **Select Columns in Dataset** to the top of the **Split Data** module, connect the lower-left circle in the **Split Data** module to the upper-right receptor in the **Train Model** module, and connect the lower-right circle to the upper-right receptor in the **Score Model** module.\n\n <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/090.png\" border=\"5\">\n\n > **Technical note:** The order of these connections (whether from the left side or the right side of the **Split Data** module) does matter.\n \n5. Run this experiment now with the split data and evaluate its $R^2$ score (right-click the bottom-center circle of the **Evaluate Model** module and select **Visualize**).\n\nOur $R^2$ score dropped to 0.79. This is a sure sign that without splitting the data, we were overfitting the model. Recall, however, that we set the random seed for our data splitting so that it would split the data the same way each time.\n\n> **Exercise**\n>\n> Return to the **Split Data** module and change the random seed value to a different number. Run the experiment a few times with different seeds and compare the $R^2$ value for the runs.\n\n> **Question**\n>\n> Why do you think that different values for the random seed would produce different $R^2$ values?\n\nClearly, some splits of the training data fit models that perform slightly better against a particular split of test data. What if we could split the data several times and see how our model does against all of those splits?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Cross validation\n\nThe idea behind cross validation is that a single set of data is split multiple times between test and training data and evaluated. Each iteration of the splitting of this data is referred to as a “fold.” For example, using 10 folds to evaluate a model is 10-fold cross validation (note that data scientists speak more generally of $k$-fold cross validation). The resulting score for a given model is the average of its performance against all of the different folds.\n\nLet's compare cross validation to regular data splitting.\n\n1. From the left pane under **Machine Learning > Initialize Model > Regression**, take another **Linear Regression** module and drag it onto the canvas somewhere to the right of your current stack of modules.\n\n2. In the **Properties** panel of this new **Linear Regression** module, make sure that you have the same settings as in the original **Linear Regression** module (L2 regularization weight = 0.001 and Random number seed = 42).\n\n3. In the left pane, under **Machine Learning > Evaluate**, click **Cross Validate Model** and drag that module onto the canvas under the new **Linear Regression** module.\n\n4. Connect the new **Linear Regression** module to the **upper-left** receptor on the **Cross Validate Model** module (the receptor does matter).\n\n5. Connect the lower-left output from the **Split Data** module to the upper-right receptor in the **Cross Validate Model** module.\n\n > **Technical note:** Although you would normally not split your data before running it though cross validation, we're doing so here to make a more apples-to-apples comparison of models evaluated with and without cross validation.\n\n6. in the **Properties** pane in the **Cross Validate Model** module, set the random seed to `42` and select the `lifeExpF` column.\n\n7. Connect the **lower-left** output from the **Cross Validate Model** module to the upper-right receptor in the **Evaluate Model** module.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/095.png\" border=\"5\">\n\n8. Run the experiment and open the evaluation screen (right-click the bottom of the **Evaluate Model** module) when all modules have finished running. (**Note:** It might take several minutes for the cross validation to run.)\n\nThe cross-validation results on are on the right. They have a lower mean absolute error, meaning that it made smaller mistakes in prediction, on average, than did the model fitted by using regular data splitting. (This is borne out by the greater number of small errors in the histogram for the cross-validation results.) That said, it has a lower $R^2$ score than the normal data-splitting on the left. This doesn't mean that it did worse; the two models are identical, were fitted using the same training data, and were compared using the same test data. What this means is that the cross-validation results are a more accurate representation of the $R^2$ score running this model against different datasets.\n\n> **Technical note:** Cross validation can given you a more consistent picture of your model's performance against new data. This is important in production applications, but one tradeoff that comes with it is computational time: your model completes an entire run for each fold of your cross validation. For computationally intense models, including many folds in cross validation might not be feasible.\n\nWhat do the results for the individual folds look like? To see the results, right-click the lower-right output circle in the **Cross Validate Model** module and select **Visualize**.\n\n<img align=\"center\" style=\"padding-right:10px;\" src=\"Images/100.png\" border=\"5\">\n\nAs you can see from the report, some folds were very good and some were spectacularly bad.\n\n> **Exercise**\n>\n> Delete the connection between the **Split Data** and **Cross Validate Model** modules and connect the output from the **Select Columns in Dataset** module to the upper-right receptor in the **Cross Validate Model** module. Re-run the experiment and review the evaluation report.\n\n> **Question**\n>\n> Why are the $R^2$ scores now much more similar?\n\nSplitting our data for training can help combat overfitting, and cross validation can give us a better idea of how our model might perform on new datasets. But what can we do about outliers in our training data?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Ridge regression\n\nOutliers in our training data -- data points that stray from the general clustering of other data points -- are problematic because they skew our model while we’re fitting it. To see this, let's look at some simplified synthetic data. Beyond pandas, we’ll also need NumPy and matplotlib to demonstrate this."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression, Ridge",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We'll create a `DataFrame` with points that are mostly regularly clustered but with two outliers added."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.random.RandomState(seed=42)\n\nN = 50\n\nX = np.linspace(0, 10, N)\ny = 0.5*X + np.random.randn(N)\n\ny[-1] += 30\ny[-2] += 30\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(X[:, np.newaxis], y)\n\nX_plot = X\ny_plot = model.predict(X_plot[:, np.newaxis])\n\nplt.scatter(X, y)\nplt.plot(X_plot, y_plot, c='orange');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The outliers have dragged our regression line off of the cluster. One way to address this is to add an additional, weighted term to the [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) equation that we used to calculate our regression line. Tuning this weight parameter for this additional term will help our regression put less emphasis on the outlying points and greater emphasis on the well-clustered points. The specific technique we'll use for this is called [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) (also referred to as L2 regularization, which is how you will see it in Azure machine learning).\n\nLet's try this again using ridge regression."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "l2_model = Ridge(alpha=1000, fit_intercept=True)\nl2_model.fit(X[:, np.newaxis], y)\n\nX_plot2 = X\ny_plot2 = l2_model.predict(X[:, np.newaxis])\n\nplt.scatter(X, y)\nplt.plot(X_plot, y_plot, c='orange')\nplt.plot(X_plot2, y_plot2, c='red');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ridge regression brought the regression line much closer to what we would expect a well-fitting regression to look like. \n\n> **Exercise**\n>\n> Rerun the code cell above with different values for the `alpha` parameter in the `Ridge()` function to see how it affects the slope of the red regression line.\n\nNow let's see ridge regression in action in our experiment in Azure ML Studio.\n\n1. Delete the **Cross Validate Model** module.\n\n2. From the left pane, drag new **Train Model** and **Score Model** modules onto the canvas under the **Linear Regression** module.\n\n3. Connect the **Linear Regression** output to the upper-left receptor in the **Train Model** module, and connect the lower-left output of the **Split Data** module to the upper-right receptor in the **Train Model** module.\n\n4. Connect **Train Model** to the upper-left receptor in the **Score Model** module, and connect the lower-right output in the **Split Data** module to the upper-right receptor in the **Score Model** module.\n\n5. Connect **Score Model** to the upper-right receptor in the **Evaluate Model** module.\n\n6. Click the **Train Model** module and select the **`lifeExpF`** column in the module's **Properties** pane.\n\n7. Click the **Linear Regression** module and enter **`42`** for the random seed. In the **L2 regularization weight** field, enter **`0.1`**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/110.png\" border=\"5\">\n \n8. Run the experiment. When all modules have finished running, view the evaluation report.\n \n> **Exercise**\n>\n> Change the L2 regularization weight on both **Linear Regression** modules to different numbers (such as `0.2` and `0.01`), and compare the new $R^2$ scores to these current ones.\n\n> **Question**\n>\n> What does this tell you about tuning your models?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Publishing a web service\n\nAfter a model has been fitted and is ready for use, it needs to be accessible. Azure ML Studio provides tools to help automate the process of turning a model into a web service.\n\n1. Delete the **Linear Regression**, **Train Model**, and **Score Model** modules that you added to the experiment in subsection above.\n\n2. After you have removed those modules, run the experiment once more. (This fits the model for the web service.)\n\n3. On the bottom of the screen, click **SET UP WEB SERVICE > Predictive Web Service [Recommended]**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/115.png\" border=\"5\">\n \n4. Azure ML Studio will now prune out the modules that are not needed for the predictive web service and will add the **Web service input** and **Web service output** modules. Connect the **Web service input** module to the **Score Model** module.\n\n5. From the left column, drag a **Select Columns in Dataset** module and place it between the **Score Model** and **Web service output** modules. Connect the modules together in this order: **Score Model -> Select Columns in Dataset -> Web service output**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/120.png\" border=\"5\">\n\n6. In the **Select Columns in Dataset** module between the **Score Model** and **Web service output** modules, select the column **Scored Labels**.\n \n7. At the bottom of the window, click **RUN**.\n\n8. After the service has completed running, click **DEPLOY WEB SERVICE** at the bottom of the window.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/125.png\" border=\"5\">\n \n9. Azure ML Studio will automatically open the predictive experiment dashboard on the **Web services** tab. Under **TEST** in the **REQUEST/RESPONSE** row, click **Test**.\n\n    <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/131.png\" border=\"5\">\n \n10. In the **Enter data to predict** window, enter data values that correspond to the columns in the dataset:\n   - **REGION** = africa\n   - **GROUP** = Africa\n   - **FERTILITY** = 3.52\n   - **LIFEEXPF** = [Because this is the column to be predicted, it does not matter what you enter here.]\n   - **PCTURBAN** = 44\n   - **LOG10(PPGDP)** = 3.8\n \n    Azure ML Studio returns `'Simple Linear Regression [Predictive Exp.]' test returned [\"47.5193162607584\"]...`\n  \n> **Exercise**\n>\n> Now try different values in the test window and see the predictions that come back.\n\n> **Takeaway**\n>\n> Web-based ML services like Azure ML Studio can speed up ML development and deployment. In particular, the drag-and-drop nature of such tools can make experimenting with different ML algorithms and settings easier. Automated tools can make deploying ML models as a web service -- a vital process in production environments -- much easier."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Accessing the web service outside of Azure ML Studio\n\nLet’s see what the experience accessing the web service you have created outside of Azure ML Studio can be like.\n\n> **Exercise**\n>\n> If you haven't already, clone or download the GitHub repo at https://github.com/microsoft/Reactors. Then run `LinRegression.py` (it's in the \"Machine_Learning_2\" folder) from the command-line interface on your local computer and enter the values of your choice. (If Python isn't installed on your computer, you can paste the code in `LinRegression.py` into a new cell in this notebook and run it from there.)\n>\n> 1.\tTo access the **`URL`** for your web service, on the Azure ML Studio web service dashboard, click **REQUEST/RESPONSE**.\n>        <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/140.png\" border=\"5\">\n>\t \n>    On the **Request Response API Documentation** page for your web service, copy the **Request URI** for the **`POST`** method for your web service.\n>        <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/145.png\" border=\"5\">\n> \n> 2.\tCopy the **`API key`** from your web service’s dashboard in Azure ML Studio.\n>        <img align=\"center\" style=\"padding-right:10px;\" src=\"Images/150.png\" border=\"5\">\n>\t \n> 3.\tValid inputs for the different dependent variables for your web service include:\n>    - **region:** `Africa`, `Asia`, `Caribbean`, `Europe`, `Latin Amer`, `North America`, `NorthAtlantic`, and `Oceania`\n>    - **group:** `africa`, `oecd`, and `other`\n>    - **fertility:** Decimal values between `1` and `7` \n>    - **gdp:** Decimal values greater than `0` (though the largest value in the model is 105095)\n>    - **pctUrban:** Integer values between `0` and `100`\n\n> **Takeaway**\n>\n> Once your ML model is published as a web service, it can be accessed from anywhere on the internet and used in a variety of ways. For more information, visit our [Data Science 2: AzureML Studio Overview](https://datascience2-sguthals.notebooks.azure.com/j/notebooks/3-AzureMLStudio-Reference.ipynb).\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}