{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Support Vector Machines (SVM)\n\nSupport Vector Machines, or SVMs, can often do better than other models at fitting data -- especially data is that is highly non-linear. Let's demonstrate by using the \"Faces in the Wild\" dataset that is provided with scikit-learn to build a facial-recognition model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load the dataset\n\nThe first step is to import facial images from the dataset. We'll set the minimum number of faces per person to 100, which means that only five sets of faces will be imported corresponding to five famous people."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_lfw_people\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfaces = fetch_lfw_people(min_faces_per_person=100)\nprint(faces.target_names)\nprint(faces.images.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In total, 1,140 facial images were loaded. Each image measures 62 by 47 pixels for a total of 2,914 pixels per image. That basically means we're working with a model with 2,914 feature columns. That's a lot of columns! Let's check the balance in our dataset by generating a histogram showing how many facial images were imported for each of the five personalities."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from collections import Counter\n\ncounts = Counter(faces.target)\nnames = {}\n\nfor key in counts.keys():\n    names[faces.target_names[key]] = counts[key]\n\ndf = pd.DataFrame.from_dict(names, orient='index')\ndf.plot(kind='bar')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The dataset is not very well balanced, but we're not too concerned about that because the net effect will probably be that our model is better at recognizing certain people than others. Let's plot some of the facial images so we can see what they look like."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fig, ax = plt.subplots(3, 5, figsize=(12, 10))\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(faces.images[i], cmap='gist_gray')\n    axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train an SVM model\n\nThe next task is to train an SVM model to do image classification using the faces in our dataset. Let's start by splitting the dataset so 80% can be used for training and 20% for testing."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(faces.data, faces.target, train_size=0.8, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's create an SVM classifier and train it using the 80% of the dataset reserved for training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.svm import SVC\n\nmodel = SVC(kernel='rbf', class_weight='balanced')\nmodel.fit(x_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, let's use the 20% of the dataset split off for testing to assess the accuracy of the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.score(x_test, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That's not very encouraging. But we're far from done."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Apply Principal Component Analysis (PCA)\n\nIt is possible that using PCA to reduce the number of columns (\"features\") will increase the accuracy of our model by filtering out the \"noise\" introduced by less-significant facial features. A pleasant side effect is that the model should train faster, too. Let's build a pipeline that performs a PCA transform on the input data, reducing 2,914 columns to 150, and uses an SVM classifier to fit a model to the training data.\n\n> **Pipelines** are a handy mechanism in scikit-learn for building complex models that transform input data before using it to train or predict."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n\npca = PCA(n_components=150, whiten=True, svd_solver='randomized', random_state=42)\nsvc = SVC(kernel='rbf', class_weight='balanced')\nmodel = make_pipeline(pca, svc)\nmodel.fit(x_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's score the model again."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.score(x_test, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That's *much* better, but we might be able to improve the accuracy even more by tuning the model's hyperparameters."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Tune the hyperparameters\n\nOne way to find the optimum combination of parameters provided to a learning algorithm in scikit-learn is to use *GridSearchCV*, which trains the model multiple times with all the different combinations of parameters that you specify. Let's use *GridSearchCV* to find the optimum values for the SVM's *C* and *gamma* parameters, which tend to have an important effect on SVM models. Note that training will take longer now because the model will be trained 16 times. (Good thing we reduced the number of dimensions by almost 95% with PCA!)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\nparams = {'svc__C': [1, 5, 10, 50],\n          'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n\ngrid = GridSearchCV(model, params)\ngrid.fit(x_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's find out what the optimum values for *C* and *gamma* are, and replace *model* with the optimized model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(grid.best_params_)\nmodel = grid.best_estimator_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, let's see if the optimized (\"hypertuned\") model does a better job of recognizing faces than our original model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.score(x_test, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It appears that we improved the model's accuracy by about 2.5%. Let's print a classification report to get a more detailed assessment of the model's accuracy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report\n\ny_predicted = model.predict(x_test)\nprint(classification_report(y_test, y_predicted, target_names=faces.target_names))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For clarity, let's generate a confusion matrix to see how the model *really* performed during testing."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\n\nmat = confusion_matrix(y_test, y_predicted)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',\n            xticklabels=faces.target_names,\n            yticklabels=faces.target_names)\nplt.xlabel('Actual label')\nplt.ylabel('Predicted label')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The model correctly identified Colin Powell 49 times out of 50, Donald Rumsfeld 23 times out of 25, and so on. That's not bad. And it's a great example of Support Vector Machines at work. It would be challenging, perhaps impossible, to do this well using more conventional learning algorithms such as logistic regression."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}