{
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "AKGfyxWhAJui"
      },
      "cell_type": "markdown",
      "source": "# *k*-means clustering\n\n*Supervised* machine learning (ML) is when we provide labeled training data for the algorithm. *k*-means clustering is an *unsupervised* ML algorithm: we provide the algorithm with a set of $n$ data points and tell the algorithm that we want that data partitioned into $k$ clusters that provide the best separation between clusters. \n\nThe *k*-means algorithm randomly assigns $k$ *centroids* (or geometric centers of subsets of the dataset as measured through [Euclidean distance](http://mathworld.wolfram.com/Distance.html)) to the data and then iteratively moves those centroids around the feature space until it converges on an arrangement of the centroids the minimizes the variance within the clusters."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![SegmentLocal|20%](Images/K-means_convergence.gif \"segment\")\n\n(Image courtesy of [Wikimedia Commons](https://en.wikipedia.org/wiki/File:K-means_convergence.gif) and is distributed under the terms of the [GNU Free Documentation License](https://en.wikipedia.org/wiki/GNU_Free_Documentation_License).)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Learning objective:** By the end of this section, you should have a basic understanding of the kinds of results the *k*-means algorithm can produce and have some idea about how you can interpret those results."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Load and prepare data\n\nTo illustrate *k*-means clustering in action, we'll use the [U.S. Department of Agriculture National Nutrient Database for Standard Reference](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/usda-national-nutrient-database-for-standard-reference/) dataset. (Note that the path name is case sensitive.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\ndf = pd.read_csv('Data/USDA-nndb-combined.csv', encoding='latin_1')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "colab_type": "code",
        "id": "6j8BELX6AJu4",
        "outputId": "494cbab7-0d6a-48b0-cb43-ce188b98dc30",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tZou0fWjS2sy"
      },
      "cell_type": "markdown",
      "source": "Because the *k*-means algorithm works with the Euclidean distance between data points, we need to separate out our descriptive from our numeric features (while saving the descriptive features for later use)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "desc_df = df.iloc[:, [0, 1, 2]+[i for i in range(50,54)]]\ndesc_df.set_index('NDB_No', inplace=True)\ndesc_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df = df.iloc[:, :-5]\nnutr_df = nutr_df.drop(['FoodGroup', 'Shrt_Desc'], axis=1)\nnutr_df.set_index('NDB_No', inplace=True)\nnutr_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The correlations that we have seen before in this dataset are still an issue here.\n\n|   | column            | row               | corr |\n|--:|------------------:|------------------:|-----:|\n| 0 | Folate\\_Tot\\_(µg) | Folate\\_DFE\\_(µg) | 0.98 |\n| 1 | Folic\\_Acid\\_(µg) | Folate\\_DFE\\_(µg) | 0.95 |\n| 2 | Folate\\_DFE\\_(µg) | Folate\\_Tot\\_(µg) | 0.98 |\n| 3 | Vit\\_A\\_RAE       | Retinol\\_(µg)     | 0.99 |\n| 4 | Retinol\\_(µg)     | Vit\\_A\\_RAE       | 0.99 |\n| 5 | Vit\\_D\\_µg        | Vit\\_D\\_IU        | 1    |\n| 6 | Vit\\_D\\_IU        | Vit\\_D\\_µg        | 1    |\n\nBefore dropping anything from the `df` `DataFrame`, however, let's take a quick look at these correlations visually. Let's start with `Folate_Tot_(µg)` and `Folate_DFE_(µg)`."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "colab_type": "code",
        "id": "SByR5hmPAJvU",
        "outputId": "620fb81d-86ca-4a58-e730-2d9471b375aa",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from matplotlib import pyplot as plt\n%matplotlib inline\nplt.scatter(df['Folate_Tot_(Âµg)'], df['Folate_DFE_(Âµg)'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is a dramatic correlation!\n\n> **Troubleshooting note:** Despite specifying Latin_1 encoding when we read in the dataset from the CSV file, character corruption can still occur. If the plt.scatter call above fails, try this code instead: `plt.scatter(df['Folate_Tot_(Âµg)'], df['Folate_DFE_(Âµg)'])`\n\n> **Exercise**\n>\n> In the code cell below, run a scatterplot of another correlative pair of features from this dataset. (**Hint:** You only need to run the `plt.scatter` function with new parameters.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's drop `Folate_DFE_(µg)`, `Vit_A_RAE`, and `Vit_D_IU` from `df` in order to eliminate these problematic correlations. The *k*-means algorithm also doesn't work with `NaN` values, so we will also have to drop those."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df.drop(['Folate_DFE_(Âµg)', 'Vit_A_RAE', 'Vit_D_IU'], \n        inplace=True, axis=1)\nnutr_df = nutr_df.dropna()\nnutr_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Troubleshooting note:** Again, if the read_csv() function did not read in the µ (mu) symbol correctly and the drop() method call above fails, try this code instead: `nutr_df.drop(['Folate_DFE_(Âµg)', 'Vit_A_RAE', 'Vit_D_IU'], inplace=True, axis=1)`\n\nBecause the *k*-means algorithm will be calculating the Euclidean distances between data points and centroids, we need to ensure that all of the numeric features in our dataset use compatible units; we don't want to try and calculate distances between units of mass like grams and units of energy like kilocalories. We will use scikit-learn's [`StandardScaler()` function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to center the data around each feature's mean and transform each feature to have a standard deviation of 1."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Hq82GRHDAJvy",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(nutr_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "e9AKLeZiP5_4"
      },
      "cell_type": "markdown",
      "source": "## Fitting the *k*-means clustering model\n\nWe are now ready to import *k*-means clustering algorithm."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "K_0y0tL6AJv6",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4Uz5zE4fP2hI"
      },
      "cell_type": "markdown",
      "source": "While we can use any number of clusters that we wish, we will initially use three clusters as a convenient number for an initial exploration of k-means clustering."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N-q1AMqGAJwC",
        "trusted": true
      },
      "cell_type": "code",
      "source": "kmeansmodel = KMeans(n_clusters=3, random_state=13)\nkmeansmodel.fit(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tCDOaubpPzkF"
      },
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> What role does our declaration of the `random_state` above play in the `KMeans()` function?\n\nLet's take a look at the labels for our clusters.\n\n> **Question**\n>\n> Given what you know about Python zero-indexing and the number of clusters we selected, what labels do you expect to see?"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "D7R__0ozAJwS",
        "outputId": "8e8c6e87-9971-40fb-8b3d-1f3e247dfa46",
        "trusted": true
      },
      "cell_type": "code",
      "source": "kmeansmodel.labels_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zS4MIorsROGa"
      },
      "cell_type": "markdown",
      "source": "As expected, our labels for the different clusters are `0`, `1`, and `2`.\n\nLet's examine the distribution of data points among the clusters."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "colab_type": "code",
        "id": "j8rF_9nFAJwZ",
        "outputId": "156ddbf3-f065-4e82-adb7-cbf642f76a87",
        "trusted": true
      },
      "cell_type": "code",
      "source": "pd.value_counts(kmeansmodel.labels_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "85gUpwR4SPlx"
      },
      "cell_type": "markdown",
      "source": "You might also check various cluster stats for comparison of the clusters.\n\nWe will want to access these labels later on, so let's add them to our `DataFrame`."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "colab_type": "code",
        "id": "kURLg3i3TVud",
        "outputId": "840c84e3-5af7-4565-9fbe-14201da5f4be",
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df['Cluster'] = kmeansmodel.labels_\nnutr_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Because data naturally clusters, we should not expect the clusters to be the same size."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Interpreting the *k*-means clustering model\n\nWhat sorts of foods populate the different clusters? We'll examine that question in greater depth later on, but we can get some sense of our clusters just by looking at the mean values for each of them."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "colab_type": "code",
        "id": "2QGRzJ8-AJwu",
        "outputId": "bd7387e4-7c90-4be4-ffe9-548b8440afee",
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df.groupby('Cluster').mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can quickly see that cluster 2 encompasses fatty foods (high `Lipid_Tot_(g)` values). Clusters 0 and 1 have similar mean protein and lipid amounts; an easy-to-see differentiation between those clusters is their relative carbohydrate and sugar levels, with those in cluster 1 being significantly higher than those in cluster 0.\n\nWe can look at these high-level differences in a little more detail using the `describe()` method, though, honestly, given the size of the `DataFrame`, this is a little cumbersome."
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "colab_type": "code",
        "id": "iWNsLalrSqCF",
        "outputId": "d1e0bbaa-d0cc-4989-f349-fb3e50178c90",
        "trusted": true
      },
      "cell_type": "code",
      "source": "nutr_df.groupby('Cluster').describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zyKQF5-CXxR8"
      },
      "cell_type": "markdown",
      "source": "We can also predict the label for new or hypothetical observations. Let's provide the nutritional details for Gjetost cheese and see which cluster our model places it in. (Gjetost cheese is one of our discarded entries from the original `DataFrame` with `NaN` values imputed below.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "newcase = [[1021,\n            13.44,\n            466.0,\n            9.65,\n            29.51,\n            4.75,\n            42.65,\n            0.0,\n            1.50,\n            400.0,\n            0.52,\n            70.0,\n            444.0,\n            1409.0,\n            600.0,\n            1.14,\n            0.08,\n            0.04,\n            14.5,\n            0.0,\n            0.315,\n            1.382,\n            0.813,\n            3.351,\n            0.271,\n            5.0,\n            0.0,\n            5.0,\n            15.4,\n            2.42,\n            1113.0,\n            334.0,\n            0,\n            20,\n            0,\n            0.5,\n            22,\n            2.4,\n            19.16,\n            7.879,\n            0.938,\n            94.0,\n            28.35]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "yo_2TROLAJxH",
        "outputId": "afc98b71-f4ac-4b9b-a002-dd42c3eac92d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "kmeansmodel.predict(newcase)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OSAmIauFZajK"
      },
      "cell_type": "markdown",
      "source": "The output indicates that Gjetost cheese falls under cluster 1.\n\n> **Exercise**\n>\n> Now go back up and alter the values for Gjetost cheese  in the `newcase` array above to see what it takes to get that array classified into cluster 0 or cluster 2.\n\nWe can also see the actual coordinate values for cluster centroids."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4y5N1dswAJxZ",
        "outputId": "b8901890-19ce-4cba-c5eb-52e384000a05",
        "trusted": true
      },
      "cell_type": "code",
      "source": "kmeansmodel.cluster_centers_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember that there is a method to this wall of numbers. Each coordinate is a point in 43-dimensional space, so each centroid is represented by an array of 43 values.\n\nLet's now add back in our descriptive features so that we can see which food groups are in our clusters."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df = nutr_df.join(desc_df)\nmerged_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can look at the value counts for the different food groups in our clusters."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df.loc[nutr_df['Cluster'] == 0]['FoodGroup'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> Look back over the Python in the code cell above. Does the syntax make sense? If not, review the documentation for [`pandas.DataFrame.loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) and [`pandas.Series.value_counts`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html).\n\n> **Exercise**\n>\n> Now find the `FoodGroup` value counts for clusters 1 and 2."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise solutions**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df.loc[nutr_df['Cluster'] == 1]['FoodGroup'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df.loc[nutr_df['Cluster'] == 2]['FoodGroup'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Some of the entries in our clusters makes sense and others look kind of like the contents of a grab bag. Part of the reason for this is that the *k*-means algorithm has to draw rather arbitrary boundaries between clusters; there will be a lot of entries in all clusters that are right on the edge and could reasonably belong to two (or more) clusters. To reduce some of this noise, we can sort these by distance from centroid of the respective clusters and look at the entries closest to the centroids."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndistances = kmeansmodel.fit_transform(X)\nmerged_df['Distance'] = np.min(distances, axis=1)\nmerged_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Questions**\n>\n> 1. Why is it only necessary to find the minimum distance to the three centroids for a given data point (`np.min`)?\n> 2. What role does the `axis=1` parameter play in the `np.min()` function?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can now sort `FoodGroup` value counts by distance to the cluster's centroid, which can make the lists of principal food types in each cluster more intuitive."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df.loc[nutr_df['Cluster'] == 0].sort_values(by='Distance')['FoodGroup'][:500].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Question**\n>\n> Look back over the Python in the code cell above. Does the syntax make sense? If not, review the documentation for [`pandas.DataFrame.sort_values`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) and [slicing ranges in pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges).\n\n> **Exercise**\n>\n> Now find the sorted `FoodGroup` value counts for clusters 1 and 2. (**Note:** Due to the differing sizes of the clusters, try a variety of ranges for your slices.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Possible exercise solutions**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df.loc[nutr_df['Cluster'] == 1].sort_values(by='Distance')['FoodGroup'][:30].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "merged_df.loc[nutr_df['Cluster'] == 2].sort_values(by='Distance')['FoodGroup'][:200].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Even without subject-matter expertise, we can gain insights of varying depth about the clusters."
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zKjocvLxZvx2"
      },
      "cell_type": "markdown",
      "source": "## Visualizing the *k*-means clustering model\n\nWe can create visualizations of our clusters, but visualization is crude at best when it has been projected down through so many dimensions. That said, even trying to project from 43 dimensions down to 3 dimensions, noticeable patterns persist. To perform this visualization, we will first need to import the Axes3D module."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\n\nmpl.rcParams['legend.fontsize'] = 10\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig)\nC = kmeansmodel.cluster_centers_\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=kmeansmodel.labels_, alpha=0.3)\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Two observations:\n> 1. Notice that there is no hard boundary visible between the purple and yellow clusters. The boundary is there, but, because it's a higher-dimensional hyperplane, it gets smeared across three dimensional space.\n> 2. Note also that points from the light blue cluster are scattered across the plot. This is also an artifact of the clusters projection down from 43 dimensions to 3."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Determining the number of clusters to use\n\nYou might have noticed that we manually determined the number $k$, the number of clusters, earlier in this section. We assigned `n_clusters_ = 3` in the model. But is 3 the right number of clusters for this dataset?\n\nIn practice, determining the correct number of clusters can require a lot on domain knowledge and data interpretation. However, we can use the elbow method as a rough heuristic for determining the number of clusters to use, even absent any domain knowledge.\n\nIn the elbow method, we plot the within-cluster sum of squares (WCSS, a measure of the variability of the observations within each cluster) against different values of $k$. We are looking for the value of $k$ near where WCSS begins to flatten out and form an elbow."
    },
    {
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QvMLOk6JAJx0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "WCSS = []\nfor num in range(1, 12):\n    kmeans = KMeans(n_clusters = num,  random_state = 50)\n    kmeans.fit(X)\n    WCSS.append(kmeans.inertia_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "colab_type": "code",
        "id": "0xUik8gBezxu",
        "outputId": "c5524b0b-4092-4003-cd3d-b435fcd04f10",
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(13, 8))\n    \nplt.plot(range(1, 12), WCSS,'ro--',linewidth=2, markersize=12)\n# plt.title('ELBOW METHOD')\nplt.rcParams.update({'font.size': 14})\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('WCSS')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4z8JhU9pgwGm"
      },
      "cell_type": "markdown",
      "source": "> **Takeaway:** Ideally, we would want to see a more pronounced elbow. One could make the case for the elbow being at $k=5$ — or at $k=8$ or $k=10$. And some datasets do produce a distinct elbow. However, real-world data is inherently messy, and there is no perfect way to evaluate *k*-means models apart from domain expertise and rigorous analysis."
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "block1_kmeans1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}